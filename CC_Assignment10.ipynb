{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ridhi004/UCS420/blob/main/CC_Assignment10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "id": "cc6ef81a-b8b0-45d1-b5a1-e06df542a56d",
      "cell_type": "code",
      "source": [
        "#q1\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Original paragraph\n",
        "text = \"\"\"Technology is evolving rapidly and reshaping every aspect of human life. From artificial intelligence to robotics, modern innovations are streamlining tasks and improving efficiency. Smartphones and cloud computing have changed the way people communicate and work. As we move forward, ethical use of technology becomes increasingly important. Education, healthcare, and transportation are just a few fields being transformed by digital solutions. The pace of change demands that we adapt and learn continuously.\"\"\"\n",
        "\n",
        "# Lowercase and remove punctuation\n",
        "text_cleaned = re.sub(r'[^\\w\\s]', '', text.lower())\n",
        "\n",
        "# Tokenize into sentences and words\n",
        "sentences = sent_tokenize(text_cleaned)\n",
        "words = word_tokenize(text_cleaned)\n",
        "\n",
        "# Split and compare\n",
        "split_words = text_cleaned.split()\n",
        "print(\"Using split():\", split_words[:10])\n",
        "print(\"Using word_tokenize():\", words[:10])\n",
        "\n",
        "# Remove stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words = [word for word in words if word not in stop_words]\n",
        "\n",
        "# Word frequency distribution\n",
        "word_freq = Counter(filtered_words)\n",
        "print(\"Word Frequency (Excluding Stopwords):\", word_freq.most_common(10))\n"
      ],
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "id": "cc6ef81a-b8b0-45d1-b5a1-e06df542a56d"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#2\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# Extract words with only alphabets\n",
        "alpha_words = re.findall(r'\\b[a-zA-Z]+\\b', text_cleaned)\n",
        "\n",
        "# Remove stopwords again from alpha words\n",
        "alpha_filtered = [word for word in alpha_words if word not in stop_words]\n",
        "\n",
        "# Stemming\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_words = [stemmer.stem(word) for word in alpha_filtered]\n",
        "\n",
        "# Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in alpha_filtered]\n",
        "\n",
        "# Display comparison\n",
        "print(\"Original (filtered):\", alpha_filtered[:10])\n",
        "print(\"Stemmed:\", stemmed_words[:10])\n",
        "print(\"Lemmatized:\", lemmatized_words[:10])\n"
      ],
      "metadata": {
        "id": "21eK3iY6tyxf"
      },
      "id": "21eK3iY6tyxf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import numpy as np\n",
        "\n",
        "texts = [\n",
        "    \"The camera quality of this phone is excellent and the battery lasts long.\",\n",
        "    \"I love the design of the laptop, but the performance could be better.\",\n",
        "    \"The sound quality of these headphones is amazing and very clear.\"\n",
        "]\n",
        "\n",
        "# Bag of Words\n",
        "cv = CountVectorizer(stop_words='english')\n",
        "bow = cv.fit_transform(texts)\n",
        "\n",
        "# TF-IDF\n",
        "tfidf = TfidfVectorizer(stop_words='english')\n",
        "tfidf_matrix = tfidf.fit_transform(texts)\n",
        "\n",
        "# Print top 3 TF-IDF keywords for each text\n",
        "feature_names = np.array(tfidf.get_feature_names_out())\n",
        "for i in range(len(texts)):\n",
        "    row = tfidf_matrix[i].toarray().flatten()\n",
        "    top3 = row.argsort()[::-1][:3]\n",
        "    print(f\"Top 3 keywords for text {i+1}:\", feature_names[top3])\n"
      ],
      "metadata": {
        "id": "P0raX6MZt4a-"
      },
      "id": "P0raX6MZt4a-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#q4\n",
        "\n",
        "text1 = \"Artificial Intelligence enables machines to learn from data and make decisions.\"\n",
        "text2 = \"Blockchain is a decentralized technology for secure and transparent transactions.\"\n",
        "\n",
        "# Preprocessing\n",
        "tokens1 = set(word_tokenize(text1.lower())) - stop_words\n",
        "tokens2 = set(word_tokenize(text2.lower())) - stop_words\n",
        "\n",
        "# Jaccard Similarity\n",
        "intersection = tokens1.intersection(tokens2)\n",
        "union = tokens1.union(tokens2)\n",
        "jaccard_similarity = len(intersection) / len(union)\n",
        "\n",
        "# Cosine Similarity\n",
        "tfidf_sim = TfidfVectorizer(stop_words='english')\n",
        "tfidf_mat = tfidf_sim.fit_transform([text1, text2])\n",
        "cos_sim = cosine_similarity(tfidf_mat[0], tfidf_mat[1])[0][0]\n",
        "\n",
        "print(\"Jaccard Similarity:\", jaccard_similarity)\n",
        "print(\"Cosine Similarity:\", cos_sim)\n"
      ],
      "metadata": {
        "id": "bjWEwzpauU_E"
      },
      "id": "bjWEwzpauU_E",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#q5\n",
        "\n",
        "from textblob import TextBlob\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sample reviews\n",
        "reviews = [\n",
        "    \"This product exceeded my expectations. The build quality is superb!\",\n",
        "    \"Terrible customer service. I'm very disappointed.\",\n",
        "    \"It's okay. Does the job, but nothing exceptional.\"\n",
        "]\n",
        "\n",
        "# Analyze sentiment\n",
        "for review in reviews:\n",
        "    blob = TextBlob(review)\n",
        "    print(f\"Review: {review}\")\n",
        "    print(f\"Polarity: {blob.sentiment.polarity}, Subjectivity: {blob.sentiment.subjectivity}\")\n",
        "    sentiment = 'Positive' if blob.sentiment.polarity > 0.1 else 'Negative' if blob.sentiment.polarity < -0.1 else 'Neutral'\n",
        "    print(f\"Sentiment: {sentiment}\\n\")\n",
        "\n",
        "# Word cloud for positive reviews\n",
        "positive_reviews = \" \".join([r for r in reviews if TextBlob(r).sentiment.polarity > 0.1])\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(positive_reviews)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title(\"Word Cloud for Positive Reviews\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "2TKZZu3auhNr"
      },
      "id": "2TKZZu3auhNr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#q6\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense\n",
        "import numpy as np\n",
        "\n",
        "# Sample training paragraph\n",
        "training_text = \"\"\"Artificial intelligence is transforming the world. It is being used in healthcare, finance, transportation, and education. AI can analyze vast amounts of data quickly. It helps in making smarter decisions. The future with AI looks promising and full of opportunities.\"\"\"\n",
        "\n",
        "# Tokenize and prepare sequences\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([training_text])\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Create input sequences\n",
        "input_sequences = []\n",
        "words = training_text.split()\n",
        "for i in range(1, len(words)):\n",
        "    n_gram_seq = tokenizer.texts_to_sequences([\" \".join(words[:i+1])])[0]\n",
        "    input_sequences.append(n_gram_seq)\n",
        "\n",
        "# Pad sequences\n",
        "input_sequences = pad_sequences(input_sequences, padding='pre')\n",
        "xs, labels = input_sequences[:, :-1], input_sequences[:, -1]\n",
        "ys = keras.utils.to_categorical(labels, num_classes=total_words)\n",
        "\n",
        "# Build LSTM model\n",
        "model = Sequential([\n",
        "    Embedding(total_words, 10, input_length=xs.shape[1]),\n",
        "    LSTM(50),\n",
        "    Dense(total_words, activation='softmax')\n",
        "])\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(xs, ys, epochs=200, verbose=0)\n",
        "\n",
        "# Text generation\n",
        "seed_text = \"AI is\"\n",
        "next_words = 5\n",
        "for _ in range(next_words):\n",
        "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen=xs.shape[1], padding='pre')\n",
        "    predicted = np.argmax(model.predict(token_list, verbose=0), axis=-1)[0]\n",
        "    output_word = \"\"\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == predicted:\n",
        "            output_word = word\n",
        "            break\n",
        "    seed_text += \" \" + output_word\n",
        "print(\"Generated Text:\", seed_text)\n"
      ],
      "metadata": {
        "id": "IBdHDP0Bumw9"
      },
      "id": "IBdHDP0Bumw9",
      "execution_count": null,
      "outputs": []
    }
  ]
}